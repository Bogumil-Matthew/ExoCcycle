{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e22ca1",
   "metadata": {},
   "source": [
    "# <center>OceanBasin_Manuscript JN</center>\n",
    "This jupyter-notebook is used to make a set of manuscript figures\n",
    "\n",
    "<center>Figure 1: ....</center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7491525",
   "metadata": {},
   "source": [
    "# Leiden Detection + Post Processing with Reconstruction (Evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229234d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#################### Imports ####################\n",
    "#################################################\n",
    "import os\n",
    "import copy as cp\n",
    "import ExoCcycle as EC\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from netCDF4 import Dataset\n",
    "import cmcrameri.cm as cmc\n",
    "import itertools\n",
    "\n",
    "# Create nodeclustering object\n",
    "from cdlib import evaluation\n",
    "from cdlib import NodeClustering\n",
    "\n",
    "\n",
    "####################################\n",
    "### Reconstruction period inputs ###\n",
    "####################################\n",
    "ages = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80];\n",
    "ages = [0]\n",
    "agestr = [ str(age) for age in ages ];\n",
    "minBasinCntV = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
    "minBasinCntV = [10]\n",
    "minBasinCntVstr = [ str(cnt) for cnt in minBasinCntV ];\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "### Define Community Detection Inputs ###\n",
    "#########################################\n",
    "# Set the detection method\n",
    "communityDetectionMethod = \"Leiden-Girvan-Newman\"\n",
    "communityDetectionMethod = \"Leiden\"\n",
    "\n",
    "# Define basin merging criteria\n",
    "mergerPackageName = \"Lite\"; # ['threshold'] = [0]\n",
    "\n",
    "# Define basin merging criteria\n",
    "mergerPackage = EC.utils.mergerPackages(mergerPackageName);\n",
    "mergerPackage['verbose'] = False;\n",
    "\n",
    "# Resolution for quality function\n",
    "resolutions = [.01]\n",
    "\n",
    "# Minimum number of basins to have in output (only used for\n",
    "# girvan-newman or composite algorithms)\n",
    "minBasinCnts = [12]\n",
    "\n",
    "# Set the ensemble size to use for the first part of the composite community detection\n",
    "# This part runs Louvain or Leiden algorithms to reduce the network complexity. Setting\n",
    "# a non-one ensemble ensures that community structure is robust given inherent randomness\n",
    "# of initial node clustering. Note that ensembles of size 100 for 1 degree resolution data\n",
    "# only increase total computational time by 1-2 minutes.\n",
    "ensembleSizes = [50];\n",
    "\n",
    "\n",
    "# Define fields to plot weights for. Note that weights must be calculated\n",
    "# for these fields.\n",
    "fieldNums = [\"Field2\"]\n",
    "\n",
    "# Show the resolution, ensembleSize, and minBasinCnt used for community detection\n",
    "# runs. Note that only one community detection is run here.\n",
    "def combine_lists(*lists):\n",
    "    \"\"\"\n",
    "    Combine N input lists into all possible unique combinations (cartesian product).\n",
    "    Returns a numpy array of shape (number_of_combinations, N).\n",
    "    \"\"\"\n",
    "    # Generate all combinations (cartesian product)\n",
    "    combinations = list(itertools.product(*lists))\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    A = np.array(combinations)\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Show the resolution, ensembleSize, and minBasinCnt used for community detection\n",
    "# runs. Note that only one community detection is run here.\n",
    "for resolution, ensembleSize, minBasinCnt, age in combine_lists(resolutions, ensembleSizes, minBasinCnts, ages):\n",
    "    print(resolution, ensembleSize, minBasinCnt, age)\n",
    "\n",
    "# Run for multiple resolution, ensembleSize, and minBasinCnt\n",
    "for resolution, ensembleSize, minBasinCnt, age in combine_lists(resolutions, ensembleSizes, minBasinCnts, ages):\n",
    "    # Change data type to avoid errors\n",
    "    minBasinCnt = int(minBasinCnt);\n",
    "    ensembleSize = int(ensembleSize);\n",
    "    age = int(age);\n",
    "    \n",
    "    # Set detection method\n",
    "    detectionMethod = {\"method\":communityDetectionMethod,\n",
    "                       \"resolution\":resolution,\n",
    "                       \"minBasinCnt\":minBasinCnt,\n",
    "                       \"ensembleSize\":ensembleSize,\n",
    "                       \"minBasinLargerThanSmallMergers\":True,\n",
    "                       \"mergerPackage\":mergerPackage}\n",
    "\n",
    "    # Set the edge weight scheme for node connections\n",
    "    # Options:\n",
    "    #    \"useGlobalDifference\", \"useEdgeDifference\", \"useEdgeGravity\"\n",
    "    #    \"useLogistic\", \"useNormPDFFittedSigmoid\", \"useQTGaussianSigmoid\"\n",
    "    #    \"useQTGaussianShiftedGaussianWeightDistribution\"\n",
    "    edgeWeightMethod = {\"method\":\"useQTGaussianShiftedGaussianWeightDistribution\",\n",
    "                       \"shortenFactor\": 5,\n",
    "                       \"shiftFactor\": .5,\n",
    "                       \"minWeight\": 0.01}\n",
    "\n",
    "    # Make folder to hold figure results\n",
    "    !mkdir -p figures/GMD_Manuscript/CodeOutputs/ReconBathymetry\n",
    "    fldName = EC.utils.makeFolderSeries(fldBase='figures/GMD_Manuscript/CodeOutputs/ReconBathymetry/Reconstruction_{}Ma-{}-PP'.format(age, communityDetectionMethod))\n",
    "    print(\"Storing images in {}\".format(fldName))\n",
    "    \n",
    "    # Short readme text to write to folder with images\n",
    "    readmetxt = \"Note that the Bathymetry values are shown with a colorbar that represents 1 std that are area weighted.\";\n",
    "    readmetxt += \"\\nUsing model S <- QTG with useQTGaussianShiftedGaussianWeightDistribution (cdfCenter  = qtDissSTD*{0} and cdfStretch = qtDissSTD/{1}) for edge weights\".format(edgeWeightMethod[\"shiftFactor\"], edgeWeightMethod[\"shortenFactor\"]);\n",
    "    readmetxt += \"\\nUsing S/distanceV edge weight\";\n",
    "    readmetxt += \"\\nWhere S = 1-CDF(difference), CDF=cumulative density function.\";\n",
    "    readmetxt += \"\\nWhere difference = values1-values2, the differnce between node property value after a Quantile Transformation values1-values2.\";\n",
    "    readmetxt += \"\\nThe CDF used for S is calculated as follows:\";\n",
    "    readmetxt += \"\\nThe absolute value of node values differences are collected into a vector (dataEdgeDiff).\";\n",
    "    readmetxt += \"\\nOutliers removed using the IQR method to make a filtered dataset (dataEdgeDiffIQRFiltered).\";\n",
    "    readmetxt += \"\\ndataEdgeDiffIQRFiltered (all positive) is mirror about 0 (symmetric about 0) is converted to a gaussian (z-score space) using a Quantile Transformation.\";\n",
    "    readmetxt += \"\\nThe distribution (dist1) of differences in gaussian (z-score space) is used to construct a CDF function\";\n",
    "    readmetxt += \"\\nA new distribution created from dist1 by offsetting it by 1 sigma_dist1 and shortening it by setting a new 1 sigma_dist2 of sigma_dist1/{}\".format(edgeWeightMethod[\"shortenFactor\"]);\n",
    "    readmetxt += \"\\nThen |difference| of node properties can be expressed in z-score space and S can be calculated as S = 1-CDF(difference)\";\n",
    "    readmetxt += \"\\nThe minimum weight for this method is set to {}\".format(edgeWeightMethod['minWeight']);\n",
    "    readmetxt += \"\\nUsing {} algorithm\".format(detectionMethod[\"method\"]);\n",
    "    readmetxt += \"\\n{} resolution: {}\".format(detectionMethod[\"method\"], detectionMethod[\"resolution\"]);\n",
    "    readmetxt += \"\\n{} ensemble size: {}\".format(detectionMethod[\"method\"], detectionMethod[\"ensembleSize\"]);\n",
    "    readmetxt += \"\\nGirvan-Newman minimum unisolated basins: {}\".format(detectionMethod['minBasinCnt']);\n",
    "    readmetxt += \"\\nCommunity merger package is EC.utils.mergePackage(package='{}')\".format(mergerPackageName);\n",
    "\n",
    "\n",
    "\n",
    "    #################################################################\n",
    "    ### Create basin object and set Field for Community detection ###\n",
    "    #################################################################\n",
    "\n",
    "    # Create basin object\n",
    "    body = [\"Earth\", \"Mars\", \"Venus\", \"Moon\"]\n",
    "    body = body[0]\n",
    "    basins = EC.utils.BasinsEA(dataDir=os.getcwd()+\"/bathymetries/{}\".format(body),\n",
    "                             filename=\"{}_resampled_1deg.nc\".format(body),\n",
    "                             body=body);\n",
    "    ####################################\n",
    "    # Add bathymetry field\n",
    "    basins.addField(resolution = basins.Fields[\"Field1\"][\"resolution\"],\n",
    "                    dataGrid =  os.getcwd()+'/PNAS_Bogumil_Results/bathymetryNCFiles/Bathymetry_{}Ma.nc'.format(age),\n",
    "                    parameter = \"z\",\n",
    "                    parameterUnit = basins.Fields[\"Field1\"][\"parameterUnit\"],\n",
    "                    parameterName = basins.Fields[\"Field1\"][\"parameterName\"])\n",
    "\n",
    "    # Assign fields to use in community detection\n",
    "    basins.useFields(fieldList=np.array([\"Field2\"]))\n",
    "\n",
    "    # Show all fields stored in basins object\n",
    "    basins.getFields(usedFields = False)\n",
    "\n",
    "    # Show all fields stored in basins object that will be used\n",
    "    # for community detection.\n",
    "    basins.getFields(usedFields = True)\n",
    "    \n",
    "    # Set field mask parameters\n",
    "    fieldMaskParameter = {\"usedField\":0, \"fliprl\":False, \"flipud\":True}\n",
    "    \n",
    "    #########################################\n",
    "    ### Run Community Detection Algorithm ###\n",
    "    #########################################\n",
    "\n",
    "    # Define basins based on user input boundaries.\n",
    "    # For the Louvain-Girvan-Newman composite algorithm the variable\n",
    "    # minBasinCnt refers to the number of basins to maintain that are\n",
    "    # not completely isolated after running the louvain algorithm.\n",
    "    basins.defineBasins(detectionMethod = detectionMethod,\n",
    "                        edgeWeightMethod = edgeWeightMethod,\n",
    "                        reducedRes={\"on\":True,\"factor\":1},\n",
    "                        read=False,\n",
    "                        write=True,\n",
    "                        verbose=False)\n",
    "\n",
    "\n",
    "    # Merge communities based off criteria \n",
    "    basins.applyMergeBasinMethods(mergerID=0, mergerPackage=mergerPackage)\n",
    "\n",
    "    # Convert basinID equal area grid to regular grid\n",
    "    basins.interp2regularGrid(mask=True)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    ### Plot results of community IDs ###\n",
    "    #####################################\n",
    "    EC.utils.plotGlobal(basins.lat, basins.lon, basins.BasinIDA,\n",
    "                        outputDir = os.getcwd()+\"/\"+fldName,\n",
    "                        fidName = \"plotGlobal.png\",\n",
    "                        cmapOpts={\"cmap\":\"jet\",\n",
    "                                  \"cbar-title\":\"cbar-title\",\n",
    "                                  \"cbar-range\":[0,np.nanmax(basins.BasinIDA)]},\n",
    "                        pltOpts={\"valueType\": \"BasinID divided by {}\".format(basins.Fields[\"Field1\"]['parameterName']),\n",
    "                                 \"valueUnits\": \"-\",\n",
    "                                 \"plotTitle\":\"\",\n",
    "                                 \"plotZeroContour\":False,\n",
    "                                 \"plotIntegerContours\":True,\n",
    "                                 \"transparent\":True},\n",
    "                        savePNG=True,\n",
    "                        saveSVG=False)\n",
    "\n",
    "    for fieldNum in fieldNums:\n",
    "        # Read reconstructed bathymetry to plot it\n",
    "        lon, lat, bathymetry, continents1 = load_netcdf_data(age)\n",
    "        lon = lon[::10].T[::10].T\n",
    "        lat = lat[::10].T[::10].T\n",
    "        bathymetry = bathymetry[::10].T[::10].T\n",
    "\n",
    "        # Calculate area weighted average and standard deviation (for plotting)\n",
    "        areaWeights, longitudes, latitudes, totalArea, totalAreaCalculated = EC.utils.areaWeights(resolution=basins.Fields[\"Field1\"]['resolution'],\n",
    "                                                                                                  LonStEd = [np.min(basins.lon),np.max(basins.lon)+basins.Fields[\"Field1\"]['resolution']],\n",
    "                                                                                                  LatStEd = [np.min(basins.lat),np.max(basins.lat)+basins.Fields[\"Field1\"]['resolution']])\n",
    "        ave, std = EC.utils.weightedAvgAndStd(basins.bathymetry, areaWeights)\n",
    "\n",
    "        #########################\n",
    "        ### Plot input fields ###\n",
    "        #########################\n",
    "        EC.utils.plotGlobal(lat, lon, bathymetry,\n",
    "                            outputDir = os.getcwd()+\"/\"+fldName,\n",
    "                            fidName = \"plotGlobal_{0}.png\".format(basins.Fields[fieldNum]['parameterName']),\n",
    "                            cmapOpts={\"cmap\":\"jet\",\n",
    "                                      \"cbar-title\":\"cbar-title\",\n",
    "                                      \"cbar-range\":[ave-1*std,\n",
    "                                                    ave+1*std]},\n",
    "                            pltOpts={\"valueType\": \"{0}\".format(basins.Fields[fieldNum]['parameterName']),\n",
    "                                     \"valueUnits\": \"{}\".format(basins.Fields[fieldNum]['parameterUnit']),\n",
    "                                     \"plotTitle\":\"\",\n",
    "                                     \"plotZeroContour\":False,\n",
    "                                     \"transparent\":True},\n",
    "                            savePNG=True,\n",
    "                            saveSVG=False)\n",
    "    \n",
    "    \n",
    "    ###########################\n",
    "    ### Plot DQT-CDF Values ###\n",
    "    ###########################\n",
    "    for fieldNum in fieldNums:\n",
    "        #####################################################\n",
    "        ####### Find average node neighbor difference #######\n",
    "        #####################################################\n",
    "        # Iterate over each node\n",
    "        attrs = None;\n",
    "        for node in basins.G.nodes:\n",
    "            # Average node connection difference for each node\n",
    "            temp = 0; cnt = 0;\n",
    "            for conNode in basins.G.neighbors(node):\n",
    "                temp+= np.abs( basins.G.nodes[conNode][fieldNum]-basins.G.nodes[node][fieldNum] )\n",
    "                cnt+=1;\n",
    "            try:\n",
    "                diffAve = temp/cnt; \n",
    "            except:\n",
    "                # If node has no connections. This rarely happens.\n",
    "                #print(temp, cnt, basins.G.neighbors(node) )\n",
    "                diffAve = 0\n",
    "            # Collect average node neighbor difference property\n",
    "            if attrs == None:\n",
    "                attrs = {node: {\"diffAve\": diffAve}};\n",
    "            else:\n",
    "                attrs[node] = {\"diffAve\": diffAve};\n",
    "\n",
    "        # Assign average node neighbor difference node property to graph\n",
    "        G = nx.set_node_attributes(basins.G, attrs)\n",
    "\n",
    "        # List values\n",
    "        diffAve_values = list(basins.G.nodes(data=\"diffAve\"))\n",
    "\n",
    "\n",
    "        #####################################################\n",
    "        ################ Interpolate to grid ################\n",
    "        #####################################################\n",
    "        def interp2regularGrid(basins, dataIrregular=None, mask=True):\n",
    "            \"\"\"\n",
    "            interp2regularGrid method is used to interpolate data to\n",
    "            a regular grid given an input of irregular spaced data.\n",
    "\n",
    "            Parameters\n",
    "            -----------\n",
    "            dataIrregular : NUMPY ARRAY\n",
    "                3XN numpy array with columns of longitude, latitude, magnitude.\n",
    "                The default is None. This will make the function define the \n",
    "                dataIrregular variable with basinIDs.\n",
    "            mask : STRING\n",
    "                The path to a netCDF4 file that can be used to mask the result\n",
    "                of interpolation. The default is None.\n",
    "\n",
    "            Returns\n",
    "            --------\n",
    "            array : NUMPY ARRAY\n",
    "                A 2nxn array that hold node properties for each corresponding\n",
    "                entry in basins.lat and basins.lon. \n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            import copy as cp\n",
    "\n",
    "            # Get basin IDs from network object.\n",
    "            tmpValuesID  = nx.get_node_attributes(basins.G, \"diffAve\");\n",
    "            tmpValuesPos = nx.get_node_attributes(basins.G, \"pos\");\n",
    "\n",
    "            # Define an array to hold longitude, latitude, and basinID\n",
    "            dataIrregular = np.zeros((len(tmpValuesPos), 3))\n",
    "\n",
    "            # Iterate over all nodes so each node's longitude, latitude,\n",
    "            # and basinID can be added to the dataIrregular array.\n",
    "            for i in tmpValuesID:\n",
    "                dataIrregular[i,:] = np.array([tmpValuesPos[i][1], tmpValuesPos[i][0], tmpValuesID[i]])\n",
    "\n",
    "            # Define an array 2nxn to hold the basin IDs for the regular grid\n",
    "            # on the surface of the a sphere (planet). \n",
    "            array = cp.deepcopy(basins.lat)\n",
    "\n",
    "            # Define a mapping function that maps node indecies on a irregular grid\n",
    "            # to those on the regular grid. This will speed up calculations if this\n",
    "            # function is called more than once.\n",
    "\n",
    "            # Iterate over all latitude and longitudes of the input grid.\n",
    "            for i in range(len(basins.lat[:,0])):\n",
    "                for j in range(len(basins.lat[0,:])):\n",
    "                    # Find the distances from each regular grid point (i,j) to all\n",
    "                    # irregular grid points.\n",
    "                    x = EC.utils.haversine_distance(lat2= dataIrregular[:,1], lat1= basins.lat[i,j],\n",
    "                                            lon2= dataIrregular[:,0], lon1= basins.lon[i,j],\n",
    "                                            radius=1)\n",
    "\n",
    "                    # Assign the nearest basin ID to element (i,j) \n",
    "                    array[i,j] = dataIrregular[np.argwhere(np.nanmin(x) == x)[0][0], 2]\n",
    "\n",
    "            ## Apply the mask\n",
    "            if mask:\n",
    "                array[np.isnan(basins.maskValue)] = np.nan\n",
    "\n",
    "\n",
    "            return array;\n",
    "\n",
    "        # Interpolate from equal area grid to regular spaced latitude/longitude\n",
    "        diffAveGrd = interp2regularGrid(basins, mask=True)\n",
    "\n",
    "        #####################################################\n",
    "        ####################### Plot ########################\n",
    "        #####################################################\n",
    "        # Transform using the QT\n",
    "        DQT_zscore = cp.deepcopy(diffAveGrd);\n",
    "        shape = np.shape(DQT_zscore)\n",
    "        DQT_zscore = np.reshape(DQT_zscore, (np.size(DQT_zscore), 1) )\n",
    "        DQT_zscoreNonNans = cp.deepcopy( DQT_zscore[~np.isnan(DQT_zscore)] )\n",
    "        DQT_zscoreNonNans =  basins.Fields[fieldNum]['weightMethodPara']['qt'].transform( np.reshape(DQT_zscoreNonNans, (len(DQT_zscoreNonNans), 1) ) )             \n",
    "        DQT_zscore[~np.isnan(DQT_zscore)] = np.reshape(DQT_zscoreNonNans, (len(DQT_zscoreNonNans),) )\n",
    "        DQT_zscore = np.reshape(DQT_zscore, shape)\n",
    "\n",
    "\n",
    "        # Plot using ExoCcycle plotGlobal function\n",
    "        EC.utils.plotGlobal(basins.lat, basins.lon, DQT_zscore,\n",
    "                            outputDir = os.getcwd()+\"/\"+fldName,\n",
    "                            fidName = \"plotGlobal_DQT_{}.png\".format(basins.Fields[fieldNum]['parameterUnit']),\n",
    "                            cmapOpts={\"cmap\":cmc.batlow,\n",
    "                                      \"cbar-title\":\"cbar-title\",\n",
    "                                      \"cbar-range\":[0,\n",
    "                                                    2]},\n",
    "                            pltOpts={\"valueType\": \"DQT\",\n",
    "                                     \"valueUnits\": \"zscore\",\n",
    "                                     \"plotTitle\":\"\",\n",
    "                                     \"plotZeroContour\":False,\n",
    "                                     \"transparent\":True},\n",
    "                            savePNG=True,\n",
    "                            saveSVG=False)\n",
    "\n",
    "    # Plot using ExoCcycle plotGlobal function\n",
    "    # Same as above except plots the weight values\n",
    "    # excluding the distance dependence between nodes.\n",
    "    # However, this should be negligible.\n",
    "    def complementCDF(diff,\n",
    "                      transformer,\n",
    "                      std,\n",
    "                      shortenFactor = edgeWeightMethod['shortenFactor'],\n",
    "                      shiftFactor = edgeWeightMethod['shiftFactor'],\n",
    "                      minWeight = edgeWeightMethod['minWeight']):\n",
    "        \"\"\"\n",
    "        complementCDF is function used to calculate the weight of a node\n",
    "        pair connection given the following inputs.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        diff : FLOAT\n",
    "            A node pair difference value from field of data.\n",
    "        transformer : OBJECT\n",
    "            Quantile transformation that is used to convert diff\n",
    "            input into z-score value.\n",
    "        std : FLAOT\n",
    "            Standard devitaion of total field data input after\n",
    "            being quantile transformed (i.e., this value should\n",
    "            be ~0). \n",
    "        shortenFactor : FLOAT\n",
    "            Factor to shorten CDF distribution by.\n",
    "        shiftFactor : FLOAT\n",
    "            Factor to shift CDF distribution by.\n",
    "        minWeight : FLOAT\n",
    "            A value between 0 and 1 that determines the minimum\n",
    "            weight to assign to a diff value. \n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        node pair edge weight(s)\n",
    "\n",
    "        \"\"\"\n",
    "        from scipy import stats\n",
    "        # Transform from diff-space to gaussian-space\n",
    "        if len(diff) == 1:\n",
    "            QTGdiff = transformer.transform( np.reshape( np.array(diff), (1,1) ) );\n",
    "        else:\n",
    "            shape = np.shape(diff)\n",
    "            QTGdiff = transformer.transform( np.reshape( np.array(diff), (np.size(diff), 1) ) )\n",
    "            QTGdiff = np.reshape(QTGdiff, shape);\n",
    "        # Get probablity in stretched distribution\n",
    "        cdfCenter  = std*shiftFactor\n",
    "        cdfStretch = std/shortenFactor\n",
    "        CDF = stats.norm.cdf(QTGdiff, loc=cdfCenter, scale=cdfStretch)\n",
    "        # Divide by probablity in normal distribution. This\n",
    "        # scales probablility between 0-1.\n",
    "        # Note that:\n",
    "        #   S->1 for |value1 - value2|-> 0   and\n",
    "        #   S->0 for |value1 - value2|-> inf\n",
    "        Ss = ( (1-CDF) + minWeight )/(minWeight+1);\n",
    "\n",
    "        return Ss\n",
    "\n",
    "    Ss= complementCDF(diffAveGrd,\n",
    "                      transformer = basins.Fields[fieldNum]['weightMethodPara']['qt'],\n",
    "                      std = basins.Fields[fieldNum]['weightMethodPara']['qtDissSTD'],\n",
    "                      shortenFactor = edgeWeightMethod['shortenFactor'],\n",
    "                      shiftFactor = edgeWeightMethod['shiftFactor'],\n",
    "                      minWeight = edgeWeightMethod['minWeight'])\n",
    "\n",
    "    EC.utils.plotGlobal(basins.lat, basins.lon, Ss,\n",
    "                        outputDir = os.getcwd()+\"/\"+fldName,\n",
    "                        fidName = \"plotGlobal_DQT_{}_weights.png\".format(basins.Fields[fieldNum]['parameterUnit']),\n",
    "                        cmapOpts={\"cmap\":cmc.batlow,\n",
    "                                  \"cbar-title\":\"cbar-title\",\n",
    "                                  \"cbar-range\":[0,\n",
    "                                                1]},\n",
    "                        pltOpts={\"valueType\": \"DQT\",\n",
    "                                 \"valueUnits\": \"Weights\",\n",
    "                                 \"plotTitle\":\"\",\n",
    "                                 \"plotZeroContour\":False,\n",
    "                                 \"transparent\":True},\n",
    "                        savePNG=True,\n",
    "                        saveSVG=False)\n",
    "    \n",
    "    ###########################################\n",
    "    ### Report community evaluation metrics ###\n",
    "    ###########################################\n",
    "    \n",
    "    # Create node cluster\n",
    "    # Note that the small basin mergers are not inlcuded\n",
    "    # in the LGNClusters. Only large basin mergers such that\n",
    "    # small basin mergers results in X chosen basins.\n",
    "    LeidenClusters=NodeClustering(communities=basins.LDcommunities,\n",
    "                           graph=basins.G,\n",
    "                           method_name=\"consensus_ledien_fixed\",\n",
    "                           method_parameters={\n",
    "                               \"resolution_parameter\": resolution,\n",
    "                               \"runs\": ensembleSize,\n",
    "                               \"distance_threshold\": 0.3}\n",
    "                          )\n",
    "\n",
    "    LGNClusters=NodeClustering(communities=basins.communitiesFinal,\n",
    "                           graph=basins.G,\n",
    "                           method_name=\"consensus_ledien_fixed\",\n",
    "                           method_parameters={\n",
    "                               \"resolution_parameter\": resolution,\n",
    "                               \"runs\": ensembleSize,\n",
    "                               \"distance_threshold\": 0.3}\n",
    "                          )\n",
    "\n",
    "\n",
    "    # Calculate community detection metrics\n",
    "    for cluster, method in zip([LeidenClusters, LGNClusters], [\"LeidenClusters\", \"LGNClusters\"]):\n",
    "        newman_girvan_modularity = evaluation.newman_girvan_modularity(basins.G, cluster)\n",
    "        internal_edge_density = evaluation.internal_edge_density(basins.G, cluster)\n",
    "        erdos_renyi_modularity= evaluation.erdos_renyi_modularity(basins.G, cluster)\n",
    "        modularity_density    = evaluation.modularity_density(basins.G, cluster)\n",
    "        avg_embeddedness      = evaluation.avg_embeddedness(basins.G, cluster)\n",
    "        conductance           = evaluation.conductance(basins.G, cluster)\n",
    "        surprise              = evaluation.surprise(basins.G, cluster)\n",
    "\n",
    "        # Add community evaluation metrics to output\n",
    "        readmetxt += \"\\n\\nCommunity evaluation metrics ({}):\\n\".format(method);\n",
    "        readmetxt += \"newman_girvan_modularity:\\t {}\\n\".format(newman_girvan_modularity.score)\n",
    "        readmetxt += \"erdos_renyi_modularity:\\t\\t {}\\n\".format(erdos_renyi_modularity.score)\n",
    "        readmetxt += \"modularity_density:\\t\\t {}\\n\".format(modularity_density.score)\n",
    "        readmetxt += \"internal_edge_density:\\t\\t {} +- {} (std)\\n\".format(internal_edge_density.score, internal_edge_density.std)\n",
    "        readmetxt += \"avg_embeddedness:\\t\\t {} +- {} (std)\\n\".format(avg_embeddedness.score, avg_embeddedness.std)\n",
    "        readmetxt += \"conductance:\\t\\t\\t {} +- {} (std)\\n\".format(conductance.score, conductance.std)\n",
    "        readmetxt += \"surprise:\\t\\t\\t {}\\n\".format(surprise.score)\n",
    "        \n",
    "    \n",
    "    with open(fldName+\"/readme.txt\", \"w\") as text_file:\n",
    "        text_file.write(readmetxt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ExoCcycle] *",
   "language": "python",
   "name": "conda-env-ExoCcycle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
