{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ca3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b311da72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8144a2b",
   "metadata": {},
   "source": [
    "### Define some spatial node network try methods on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488cf357",
   "metadata": {},
   "source": [
    "# Different Communitity Detection Methods Below "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6602f699",
   "metadata": {},
   "source": [
    "### Liedian methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8121dddb",
   "metadata": {},
   "source": [
    "#### Liedian method (ensemble - highest modularity)\n",
    "\n",
    "Note that there is no average or consensus taken on the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66293adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import leidenalg\n",
    "from cdlib import NodeClustering, evaluation, ensemble\n",
    "\n",
    "# Custom Leiden wrapper that mimics a CDlib algorithm\n",
    "def leiden_resolution_weighted(graph_nx, resolution_parameter):\n",
    "    \"\"\"\n",
    "    Custom Leiden wrapper for ensemble that supports resolution_parameter and edge weights.\n",
    "    Assumes edge weights are stored under the attribute 'bathyAve'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert NetworkX to igraph and include weights\n",
    "    edges = [(u, v, d.get(\"bathyAve\", 1.0)) for u, v, d in graph_nx.edges(data=True)]\n",
    "    g_ig = ig.Graph.TupleList(edges, directed=False, weights=True)\n",
    "\n",
    "    # Run Leiden with resolution and weights\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_ig,\n",
    "        leidenalg.RBConfigurationVertexPartition,\n",
    "        resolution_parameter=resolution_parameter,\n",
    "        weights=g_ig.es[\"weight\"]\n",
    "    )\n",
    "\n",
    "    # Extract communities (assume numeric node names)\n",
    "    communities = [set(g_ig.vs[idx][\"name\"] if \"name\" in g_ig.vs.attributes() else idx for idx in comm)\n",
    "                   for comm in partition]\n",
    "\n",
    "    return NodeClustering(\n",
    "        communities=communities,\n",
    "        graph=graph_nx,\n",
    "        method_name=\"leiden_weighted\",\n",
    "        method_parameters={\"resolution_parameter\": resolution_parameter}\n",
    "    )\n",
    "\n",
    "# Define ensemble parameter to sweep\n",
    "resolution = ensemble.Parameter(name=\"resolution_parameter\", start=30, end=30.1, step=0.1)\n",
    "\n",
    "# Run ensemble grid search with custom Leiden wrapper\n",
    "ensembleLen = 100;\n",
    "configurations = [];\n",
    "methods = [];\n",
    "for i in range(ensembleLen):\n",
    "    methods.append(leiden_resolution_weighted);\n",
    "    configurations.append([resolution]);\n",
    "#print(methods)\n",
    "#print(configurations)\n",
    "highScore = 0\n",
    "for communities, score in ensemble.pool_grid_filter(\n",
    "    basins.G,\n",
    "    methods=methods,\n",
    "    configurations=configurations,\n",
    "    quality_score=evaluation.erdos_renyi_modularity,\n",
    "    aggregate=max\n",
    "):\n",
    "    print(\"Score: {0}, Communities: {1}\".format(score, len(communities.communities)))\n",
    "    # Keep communities from the highest scoring model\n",
    "    if highScore < score.score: \n",
    "        highScore = cp.deepcopy(score.score);\n",
    "        communitiesFinal = cp.deepcopy(communities);\n",
    "        \n",
    "print(\"Score: {0}, Communities: {1}\".format(highScore, len(communitiesFinal.communities)))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#basins.G\n",
    "\n",
    "#LDcommunities\n",
    "pos = np.array([])\n",
    "\n",
    "cnt=0\n",
    "for node in basins.G.nodes(data=True):\n",
    "    #print( np.array(node[1]['pos']) )\n",
    "    posi = np.array(node[1]['pos'])\n",
    "    pos = np.append(pos, posi)\n",
    "    \n",
    "pos = pos.reshape(len(pos)//2, 2)\n",
    "basinID = np.ones(len(pos))\n",
    "basinID[:] = np.nan;\n",
    "for node in basins.G.nodes(data=True):\n",
    "    cntCom = 0\n",
    "    for comidx in range(len(communitiesFinal.communities)):\n",
    "        if node[0] in communitiesFinal.communities[comidx]:\n",
    "            basinID[node[0]] = cntCom\n",
    "        elif comidx == len(communitiesFinal.communities):\n",
    "            basinID = np.append(basinID, np.nan)\n",
    "        cntCom+=1\n",
    "\n",
    "\n",
    "\n",
    "len(pos), len(basinID), np.sum(np.isnan(basinID))\n",
    "\n",
    "plt.scatter(pos[:,1], pos[:,0], c=basinID, s=1, cmap='jet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563801a",
   "metadata": {},
   "source": [
    "#### Liedian method (ensemble - consensus)\n",
    "\n",
    "Note that there is no average or highest modularity included in the consensus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b694ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import leidenalg\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from cdlib import NodeClustering\n",
    "\n",
    "def consensus_leiden(graph_nx, resolution_parameter=1.0, weight_attr=\"bathyAve\", runs=20, distance_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Consensus clustering from multiple Leiden runs with proper node name handling and configurable threshold.\n",
    "    \"\"\"\n",
    "    # Stable node ordering\n",
    "    nodes = sorted(graph_nx.nodes())\n",
    "    n = len(nodes)\n",
    "    node_to_idx = {node: i for i, node in enumerate(nodes)}\n",
    "    idx_to_node = {i: node for node, i in node_to_idx.items()}\n",
    "\n",
    "    # Build weighted edge list with consistent node labels\n",
    "    edges = [(node_to_idx[u], node_to_idx[v], d.get(weight_attr, 1.0)) for u, v, d in graph_nx.edges(data=True)]\n",
    "    g = ig.Graph()\n",
    "    g.add_vertices(n)\n",
    "    g.add_edges([(u, v) for u, v, w in edges])\n",
    "    g.es[\"weight\"] = [w for _, _, w in edges]\n",
    "    g.vs[\"name\"] = list(range(n))  # Stable index-named nodes\n",
    "\n",
    "    # Initialize co-association matrix\n",
    "    coassoc = np.zeros((n, n))\n",
    "\n",
    "    for i in range(runs):\n",
    "        part = leidenalg.find_partition(\n",
    "            g,\n",
    "            leidenalg.RBConfigurationVertexPartition,\n",
    "            resolution_parameter=resolution_parameter,\n",
    "            weights=g.es[\"weight\"],\n",
    "            seed=i\n",
    "        )\n",
    "        for community in part:\n",
    "            for u in community:\n",
    "                for v in community:\n",
    "                    coassoc[u, v] += 1\n",
    "\n",
    "    # Normalize co-association matrix\n",
    "    coassoc /= runs\n",
    "\n",
    "    # Convert to dissimilarity for clustering\n",
    "    distance = 1.0 - coassoc\n",
    "\n",
    "    # Use Agglomerative Clustering with better threshold control\n",
    "    model = AgglomerativeClustering(\n",
    "        metric=\"precomputed\",\n",
    "        linkage=\"average\",\n",
    "        distance_threshold=distance_threshold,\n",
    "        n_clusters=None\n",
    "    )\n",
    "    labels = model.fit_predict(distance)\n",
    "\n",
    "    # Group nodes by cluster labels\n",
    "    consensus_communities = [[] for _ in range(max(labels)+1)]\n",
    "    for idx, label in enumerate(labels):\n",
    "        consensus_communities[label].append(idx_to_node[idx])\n",
    "\n",
    "    # Convert to sets\n",
    "    consensus_communities = [set(c) for c in consensus_communities]\n",
    "\n",
    "    return NodeClustering(\n",
    "        communities=consensus_communities,\n",
    "        graph=graph_nx,\n",
    "        method_name=\"consensus_leiden_fixed\",\n",
    "        method_parameters={\n",
    "            \"resolution_parameter\": resolution_parameter,\n",
    "            \"runs\": runs,\n",
    "            \"distance_threshold\": distance_threshold\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "communitiesFinal = consensus_leiden(basins.G,\n",
    "                                          resolution_parameter=0.6,\n",
    "                                          distance_threshold=0.3,\n",
    "                                          runs=50)\n",
    "print(f\"Consensus communities found: {len(communitiesFinal.communities)}\")\n",
    "\n",
    "\n",
    "\n",
    "pos = np.array([])\n",
    "\n",
    "cnt=0\n",
    "for node in basins.G.nodes(data=True):\n",
    "    #print( np.array(node[1]['pos']) )\n",
    "    posi = np.array(node[1]['pos'])\n",
    "    pos = np.append(pos, posi)\n",
    "    \n",
    "pos = pos.reshape(len(pos)//2, 2)\n",
    "basinID = np.ones(len(pos))\n",
    "basinID[:] = np.nan;\n",
    "for node in basins.G.nodes(data=True):\n",
    "    cntCom = 0\n",
    "    for comidx in range(len(communitiesFinal.communities)):\n",
    "        if node[0] in communitiesFinal.communities[comidx]:\n",
    "            basinID[node[0]] = cntCom\n",
    "        elif comidx == len(communitiesFinal.communities):\n",
    "            basinID = np.append(basinID, np.nan)\n",
    "        cntCom+=1\n",
    "\n",
    "\n",
    "len(pos), len(basinID), np.sum(np.isnan(basinID))\n",
    "\n",
    "plt.scatter(pos[:,1], pos[:,0], c=basinID, s=1, cmap='jet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b69d4",
   "metadata": {},
   "source": [
    "## infomap methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226e1c2",
   "metadata": {},
   "source": [
    "#### Composite: infomap + girvan-newman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class compositeAl():\n",
    "    \n",
    "    \n",
    "    def __init__(self, basins):\n",
    "        \n",
    "        self.G = basins.G\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        # Hierarchical community detection method \n",
    "        #\n",
    "        # Imports\n",
    "        from collections import defaultdict\n",
    "        import itertools\n",
    "        from cdlib import algorithms\n",
    "\n",
    "        communities = algorithms.infomap(basins.G)\n",
    "\n",
    "\n",
    "        def lists_to_cells(nested_list):\n",
    "            \"\"\"\n",
    "            Convert a list of lists into a list of sets (cells).\n",
    "\n",
    "            Parameters:\n",
    "                nested_list (list of list): A list where each inner list represents a group or community.\n",
    "\n",
    "            Returns:\n",
    "                list of set: A list of sets, where each set corresponds to a community.\n",
    "            \"\"\"\n",
    "            return [set(group) for group in nested_list]\n",
    "\n",
    "        LDcommunities = lists_to_cells(communities.communities)\n",
    "\n",
    "        self.LDcommunities = cp.deepcopy(LDcommunities)\n",
    "        self.LDcommunitiesUnaltered = cp.deepcopy(LDcommunities);\n",
    "\n",
    "        self.communitiesFinal = self.LDcommunities;\n",
    "\n",
    "\n",
    "\n",
    "        ## Mapping from node to community index from Louvain community detection\n",
    "        node_to_comm = {}\n",
    "        for idx, comm in enumerate(LDcommunities):\n",
    "            for node in comm:\n",
    "                node_to_comm[node] = idx\n",
    "\n",
    "\n",
    "        # Construct new graph with Louvain community consolidated nodes\n",
    "        self.Gnew = nx.Graph()\n",
    "\n",
    "        # Add *all* communities as nodes, even if disconnected\n",
    "        self.Gnew.add_nodes_from(range(len(LDcommunities)))  # One node per community index\n",
    "\n",
    "        # Track summed weights between communities\n",
    "        edge_weights = defaultdict(float)\n",
    "\n",
    "        # Track unisolated louvain communities (communities that connect to other communities).\n",
    "        unisolatedCommunities = np.array([]);\n",
    "        smallCommunities = np.array([]);\n",
    "        if not minBasinLargerThanSmallMergers:\n",
    "            # Iterate over all edges in the original graph\n",
    "            for u, v, data in self.G.edges(data=True):\n",
    "                cu = node_to_comm[u]\n",
    "                cv = node_to_comm[v]\n",
    "                weight = data.get('bathyAve', 1.0)\n",
    "\n",
    "                if cu != cv:\n",
    "                    # Undirected: sort community pair to avoid duplicates\n",
    "                    edge = tuple(sorted((cu, cv)))\n",
    "                    edge_weights[edge] += weight\n",
    "\n",
    "                    # Tracks louvain community ids that connect to other communities\n",
    "                    if (unisolatedCommunities != cu).all() | (len(unisolatedCommunities)==0):\n",
    "                        unisolatedCommunities = np.append(unisolatedCommunities, cu)\n",
    "        elif minBasinLargerThanSmallMergers:\n",
    "            print(\"\\n\\n\\n\\nminBasinLargerThanSmallMergers1\\n\\n\\n\\n\")\n",
    "            # Get the area weights and basinID\n",
    "            # area = nx.get_node_attributes(self.G, \"areaWeightm2\")\n",
    "\n",
    "            # basinID = nx.get_node_attributes(self.G, \"basinID\")\n",
    "\n",
    "            # basinIDList = np.array( [basinID[idx]['basinID'] for idx in nx.get_node_attributes(self.G, \"basinID\")] )\n",
    "            # areaList = np.array( [area[idx] for idx in nx.get_node_attributes(self.G, \"basinID\")] )\n",
    "\n",
    "            # # Sum areas with same basinIDs.\n",
    "            # sumCommunities = np.zeros(len(np.unique(basinIDList)))\n",
    "            # for i in range(len(np.unique(basinIDList))):\n",
    "            #     sumCommunities[int(i)] = np.sum(areaList[i==basinIDList])\n",
    "\n",
    "            # Get the area weights and basinID\n",
    "            area = nx.get_node_attributes(self.G, \"areaWeightm2\")\n",
    "            areaList = np.array( [area[idx] for idx in nx.get_node_attributes(self.G, \"areaWeightm2\")] )\n",
    "\n",
    "            # Sum areas with same basinIDs.\n",
    "            sumCommunities = np.zeros(len(self.LDcommunities))\n",
    "            for i in range(len(sumCommunities)):\n",
    "                sumCommunities[int(i)] = np.sum( areaList[ np.array( list(self.LDcommunities[i]) ) ] )\n",
    "\n",
    "            if detectionMethod['mergerPackage']['mergeSmallBasins']['thresholdMethod'] == \"%\":\n",
    "                # Using % of spatial graph area\n",
    "\n",
    "                # Define in percentage of total graph area.\n",
    "                sumCommunitiesPercentage = 100*sumCommunities/np.sum(sumCommunities)\n",
    "\n",
    "                # Make list of communities that are larger than the smallest merged community\n",
    "                smallCommunities = ( sumCommunitiesPercentage>np.max(detectionMethod['mergerPackage']['mergeSmallBasins']['threshold']) )\n",
    "\n",
    "                # print(\"\\n\\n\\n\\nminBasinLargerThanSmallMergers2\\n\\n\\n\\n\")\n",
    "                # print(\"np.max(detectionMethod['mergerPackage']['mergeSmallBasins']['threshold'])\\n\", np.max(detectionMethod['mergerPackage']['mergeSmallBasins']['threshold']))\n",
    "                # print(\"\\nsumCommunitiesPercentage\\n\",sumCommunitiesPercentage)\n",
    "            else:\n",
    "                # Using absolute values of spatial graph area (i.e., m2)\n",
    "\n",
    "                # Make list of communities that are larger than the smallest merged community\n",
    "                smallCommunities = ( sumCommunities>np.max(detectionMethod['mergerPackage']['mergeSmallBasins']['threshold']) )\n",
    "\n",
    "\n",
    "        # Communities that share no edge with other community\n",
    "        # Used for determining the number of unisolated communities\n",
    "        # when using the girvan-newman algorithm.\n",
    "        # print(\"\\n\\n\\n\\nsum(unisolatedCommunities): {}\\n\\n\\n\\n\".format(np.sum(unisolatedCommunities)))\n",
    "        # print(\"\\n\\n\\n\\nunisolatedCommunities: {}\\n\\n\\n\\n\".format(unisolatedCommunities) )\n",
    "        isolatedCommunitiesCnt = len(LDcommunities)- len(unisolatedCommunities)\n",
    "\n",
    "        # Add weighted edges to Gnew\n",
    "        for (cu, cv), edge_weight in edge_weights.items():\n",
    "            self.Gnew.add_edge(cu, cv, bathyAve=edge_weight)\n",
    "\n",
    "        # Apply Girvan–Newman algorithm to the simplified community graph\n",
    "        communityCnt = isolatedCommunitiesCnt + minBasinCnt\n",
    "        print(\"Louvain Communities ({0}), Target ({1}), Isolated Communities {2}\".format(len(LDcommunities),communityCnt, isolatedCommunitiesCnt))\n",
    "        import time\n",
    "\n",
    "        timestamp1 = time.time();\n",
    "        comp = nx.community.girvan_newman(self.Gnew, most_valuable_edge=mostCentralEdge)\n",
    "        print( \"Time: {} seconds\".format(timestamp1-time.time()) ); timestamp1 = time.time();\n",
    "        limited = itertools.takewhile(lambda c: len(c) <= communityCnt, comp)\n",
    "        print( \"Time: {} seconds\".format(timestamp1-time.time()) ); timestamp1 = time.time();\n",
    "        for communities in limited:\n",
    "            GNcommunities = communities\n",
    "        print( \"Time: {} seconds\".format(timestamp1-time.time()) ); timestamp1 = time.time();\n",
    "        self.GNcommunities = GNcommunities\n",
    "\n",
    "        # Map each Girvan–Newman community to its Louvain community\n",
    "        louvain_to_gn = {}\n",
    "        for idx, comm in enumerate(GNcommunities):\n",
    "            for c in comm:\n",
    "                louvain_to_gn[c] = idx\n",
    "\n",
    "        # Map each original node to a Girvan–Newman community via its Louvain community\n",
    "        print( \"Time: {} seconds\".format(timestamp1-time.time()) ); timestamp1 = time.time();\n",
    "        commNodes = [{} for _ in range(len(LDcommunities))]\n",
    "        for commL in louvain_to_gn:\n",
    "            commGN = louvain_to_gn[commL];\n",
    "\n",
    "            #print(LDcommunities[commL])\n",
    "            try:\n",
    "                # Do not comment out. If this code can run then commNodes[commGN]\n",
    "                # has already been defined\n",
    "                len(commNodes[commGN]);\n",
    "                commNodes[commGN].update(LDcommunities[commL])\n",
    "            except:\n",
    "                commNodes[commGN] = LDcommunities[commL]\n",
    "\n",
    "        # Redefine the node community structure using Louvain & Girvan Newman composite communities\n",
    "        self.communitiesFinal = commNodes;\n",
    "\n",
    "        print( \"Time: {} seconds\".format(timestamp1-time.time()) ); timestamp1 = time.time();\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ExoCcycle] *",
   "language": "python",
   "name": "conda-env-ExoCcycle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
